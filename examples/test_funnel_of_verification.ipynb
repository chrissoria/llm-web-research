{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "# Test Funnel of Verification (FoVe) Functions\n\nThis notebook tests the Funnel of Verification functions across all three providers:\n1. `funnel_of_verification_anthropic` - Claude with web_search tool\n2. `funnel_of_verification_google` - Gemini with grounded search\n3. `funnel_of_verification_perplexity` - Perplexity with web search\n\n## FoVe Pipeline Overview\n- **Step 1**: Gather broad information about the item (web search)\n- **Step 2**: Extract concise answer from context (no web search) - can trigger early exit\n- **Step 3**: Skeptically verify the answer (web search)\n- **Step 4**: Format output as strict JSON (no web search)\n\n## Test Goal\nTest with **1 search question** and **5 company inputs** to verify structured DataFrame output with columns: `search_input`, `answer`, `url`, `confidence`, `multiple_entities`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Use local src\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
    "sys.path.insert(0, src_path)\n",
    "\n",
    "from llm_web_research.calls import (\n",
    "    funnel_of_verification_anthropic,\n",
    "    funnel_of_verification_google,\n",
    "    funnel_of_verification_perplexity\n",
    ")\n",
    "\n",
    "print(\"Funnel of Verification functions loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-api-keys",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from .env file\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "os.chdir('/Users/chrissoria/Documents/Research/Categorization_AI_experiments')\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "perplexity_api_key = os.getenv(\"PERPLEXITY_API_KEY\")\n",
    "\n",
    "# Change back to llm-web-research directory\n",
    "os.chdir('/Users/chrissoria/Documents/Research/llm-web-research')\n",
    "\n",
    "# Verify keys loaded\n",
    "print(\"API keys loaded:\")\n",
    "print(f\"  Anthropic: {'Y' if anthropic_api_key else 'N'}\")\n",
    "print(f\"  Google: {'Y' if google_api_key else 'N'}\")\n",
    "print(f\"  Perplexity: {'Y' if perplexity_api_key else 'N'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-params",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parameters: 1 question, 5 companies\n",
    "search_question = \"current CEO\"\n",
    "answer_format = \"name\"\n",
    "search_inputs = [\"Microsoft\", \"Apple Inc\", \"Amazon\", \"Google\", \"Meta\"]\n",
    "\n",
    "# Common parameters\n",
    "additional_instructions = \"\"\n",
    "creativity = 0\n",
    "time_delay = 3  # seconds between requests\n",
    "\n",
    "print(f\"Search question: {search_question}\")\n",
    "print(f\"Answer format: {answer_format}\")\n",
    "print(f\"\\nCompanies to search ({len(search_inputs)}):\")\n",
    "for i, item in enumerate(search_inputs, 1):\n",
    "    print(f\"  {i}. {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anthropic-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 1: Anthropic (Claude with web_search tool)\n",
    "\n",
    "Uses `tool_choice` with `input_schema` to force JSON output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-anthropic",
   "metadata": {},
   "outputs": [],
   "source": "if anthropic_api_key:\n    import anthropic\n    \n    print(\"Testing Funnel of Verification with Anthropic...\\n\")\n    client = anthropic.Anthropic(api_key=anthropic_api_key)\n    user_model = \"claude-sonnet-4-20250514\"\n    \n    results = []\n    \n    for idx, item in enumerate(tqdm(search_inputs, desc=\"Processing\")):\n        if idx > 0:\n            time.sleep(time_delay)\n        \n        print(f\"\\n{'#'*80}\")\n        print(f\"# PROCESSING: {item}\")\n        print(f\"{'#'*80}\")\n        \n        try:\n            result = funnel_of_verification_anthropic(\n                item=item,\n                search_question=search_question,\n                answer_format=answer_format,\n                additional_instructions=additional_instructions,\n                client=client,\n                user_model=user_model,\n                creativity=creativity,\n                verbose=True  # Enable verbose output\n            )\n            \n            # Parse JSON result\n            parsed = json.loads(result)\n            results.append({\n                'search_input': item,\n                'answer': parsed.get('answer', ''),\n                'url': parsed.get('url', ''),\n                'confidence': parsed.get('confidence', ''),\n                'multiple_entities': parsed.get('multiple_entities', '0')\n            })\n            \n        except json.JSONDecodeError as e:\n            print(f\"JSON parse error for {item}: {e}\")\n            print(f\"Raw result: {result}\")\n            results.append({\n                'search_input': item,\n                'answer': f'JSON Error: {result[:100]}',\n                'url': '',\n                'confidence': '0',\n                'multiple_entities': '0'\n            })\n        except Exception as e:\n            print(f\"Error for {item}: {e}\")\n            results.append({\n                'search_input': item,\n                'answer': f'Error: {e}',\n                'url': '',\n                'confidence': '0',\n                'multiple_entities': '0'\n            })\n    \n    # Create DataFrame\n    df_anthropic = pd.DataFrame(results)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"ANTHROPIC RESULTS\")\n    print(\"=\"*60)\n    print(f\"\\nDataFrame shape: {df_anthropic.shape}\")\n    print(f\"Columns: {list(df_anthropic.columns)}\")\n    print(\"\\n\")\n    display(df_anthropic)\nelse:\n    print(\"Skipping: Anthropic API key not found\")"
  },
  {
   "cell_type": "markdown",
   "id": "google-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 2: Google (Gemini with grounded search)\n",
    "\n",
    "Uses `responseMimeType: application/json` to force JSON output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-google",
   "metadata": {},
   "outputs": [],
   "source": "if google_api_key:\n    import requests\n    \n    print(\"Testing Funnel of Verification with Google...\\n\")\n    \n    user_model = \"gemini-2.5-flash\"\n    url = f\"https://generativelanguage.googleapis.com/v1beta/models/{user_model}:generateContent\"\n    headers = {\n        \"x-goog-api-key\": google_api_key,\n        \"Content-Type\": \"application/json\"\n    }\n    \n    def make_google_request(url, headers, payload, max_retries=5):\n        \"\"\"Make Google API request with retry logic.\"\"\"\n        for attempt in range(max_retries):\n            response = requests.post(url, headers=headers, json=payload)\n            if response.status_code == 429:\n                wait_time = 2 ** attempt\n                print(f\"  Rate limited, waiting {wait_time}s...\")\n                time.sleep(wait_time)\n                continue\n            response.raise_for_status()\n            return response.json()\n        raise Exception(\"Max retries exceeded\")\n    \n    results = []\n    \n    for idx, item in enumerate(tqdm(search_inputs, desc=\"Processing\")):\n        if idx > 0:\n            time.sleep(time_delay)\n        \n        print(f\"\\n{'#'*80}\")\n        print(f\"# PROCESSING: {item}\")\n        print(f\"{'#'*80}\")\n        \n        try:\n            result = funnel_of_verification_google(\n                item=item,\n                search_question=search_question,\n                answer_format=answer_format,\n                additional_instructions=additional_instructions,\n                url=url,\n                headers=headers,\n                creativity=creativity,\n                make_google_request=make_google_request,\n                verbose=True  # Enable verbose output\n            )\n            \n            # Parse JSON result\n            parsed = json.loads(result)\n            results.append({\n                'search_input': item,\n                'answer': parsed.get('answer', ''),\n                'url': parsed.get('url', ''),\n                'confidence': parsed.get('confidence', ''),\n                'multiple_entities': parsed.get('multiple_entities', '0')\n            })\n            \n        except json.JSONDecodeError as e:\n            print(f\"JSON parse error for {item}: {e}\")\n            print(f\"Raw result: {result}\")\n            results.append({\n                'search_input': item,\n                'answer': f'JSON Error: {result[:100]}',\n                'url': '',\n                'confidence': '0',\n                'multiple_entities': '0'\n            })\n        except Exception as e:\n            print(f\"Error for {item}: {e}\")\n            results.append({\n                'search_input': item,\n                'answer': f'Error: {e}',\n                'url': '',\n                'confidence': '0',\n                'multiple_entities': '0'\n            })\n    \n    # Create DataFrame\n    df_google = pd.DataFrame(results)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"GOOGLE RESULTS\")\n    print(\"=\"*60)\n    print(f\"\\nDataFrame shape: {df_google.shape}\")\n    print(f\"Columns: {list(df_google.columns)}\")\n    print(\"\\n\")\n    display(df_google)\nelse:\n    print(\"Skipping: Google API key not found\")"
  },
  {
   "cell_type": "markdown",
   "id": "perplexity-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 3: Perplexity\n",
    "\n",
    "Uses `response_format` with `json_schema` to force JSON output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-perplexity",
   "metadata": {},
   "outputs": [],
   "source": "if perplexity_api_key:\n    from perplexity import Perplexity\n    \n    print(\"Testing Funnel of Verification with Perplexity...\\n\")\n    client = Perplexity(api_key=perplexity_api_key)\n    user_model = \"sonar\"\n    \n    results = []\n    \n    for idx, item in enumerate(tqdm(search_inputs, desc=\"Processing\")):\n        if idx > 0:\n            time.sleep(time_delay)\n        \n        print(f\"\\n{'#'*80}\")\n        print(f\"# PROCESSING: {item}\")\n        print(f\"{'#'*80}\")\n        \n        try:\n            result = funnel_of_verification_perplexity(\n                item=item,\n                search_question=search_question,\n                answer_format=answer_format,\n                additional_instructions=additional_instructions,\n                client=client,\n                user_model=user_model,\n                creativity=creativity,\n                verbose=True  # Enable verbose output\n            )\n            \n            # Parse JSON result\n            parsed = json.loads(result)\n            results.append({\n                'search_input': item,\n                'answer': parsed.get('answer', ''),\n                'url': parsed.get('url', ''),\n                'confidence': parsed.get('confidence', ''),\n                'multiple_entities': parsed.get('multiple_entities', '0')\n            })\n            \n        except json.JSONDecodeError as e:\n            print(f\"JSON parse error for {item}: {e}\")\n            print(f\"Raw result: {result}\")\n            results.append({\n                'search_input': item,\n                'answer': f'JSON Error: {result[:100]}',\n                'url': '',\n                'confidence': '0',\n                'multiple_entities': '0'\n            })\n        except Exception as e:\n            print(f\"Error for {item}: {e}\")\n            results.append({\n                'search_input': item,\n                'answer': f'Error: {e}',\n                'url': '',\n                'confidence': '0',\n                'multiple_entities': '0'\n            })\n    \n    # Create DataFrame\n    df_perplexity = pd.DataFrame(results)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"PERPLEXITY RESULTS\")\n    print(\"=\"*60)\n    print(f\"\\nDataFrame shape: {df_perplexity.shape}\")\n    print(f\"Columns: {list(df_perplexity.columns)}\")\n    print(\"\\n\")\n    display(df_perplexity)\nelse:\n    print(\"Skipping: Perplexity API key not found\")"
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Results Comparison\n",
    "\n",
    "Side-by-side comparison of all three providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comparison DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "for item in search_inputs:\n",
    "    row = {'company': item}\n",
    "    \n",
    "    # Anthropic\n",
    "    if 'df_anthropic' in dir():\n",
    "        match = df_anthropic[df_anthropic['search_input'] == item]\n",
    "        if not match.empty:\n",
    "            row['anthropic_answer'] = match.iloc[0]['answer']\n",
    "            row['anthropic_conf'] = match.iloc[0]['confidence']\n",
    "    \n",
    "    # Google\n",
    "    if 'df_google' in dir():\n",
    "        match = df_google[df_google['search_input'] == item]\n",
    "        if not match.empty:\n",
    "            row['google_answer'] = match.iloc[0]['answer']\n",
    "            row['google_conf'] = match.iloc[0]['confidence']\n",
    "    \n",
    "    # Perplexity\n",
    "    if 'df_perplexity' in dir():\n",
    "        match = df_perplexity[df_perplexity['search_input'] == item]\n",
    "        if not match.empty:\n",
    "            row['perplexity_answer'] = match.iloc[0]['answer']\n",
    "            row['perplexity_conf'] = match.iloc[0]['confidence']\n",
    "    \n",
    "    comparison_data.append(row)\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARISON: CEO Answers Across Providers\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nQuestion: '{search_question}'\")\n",
    "print(f\"Format: '{answer_format}'\\n\")\n",
    "display(df_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreement-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check agreement between providers\n",
    "print(\"\\nAgreement Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'df_comparison' in dir() and len(df_comparison) > 0:\n",
    "    answer_cols = [c for c in df_comparison.columns if '_answer' in c]\n",
    "    \n",
    "    if len(answer_cols) >= 2:\n",
    "        for idx, row in df_comparison.iterrows():\n",
    "            answers = [row.get(c, 'N/A') for c in answer_cols if pd.notna(row.get(c))]\n",
    "            unique_answers = set(a.lower().strip() for a in answers if a and a != 'N/A')\n",
    "            \n",
    "            if len(unique_answers) == 1:\n",
    "                status = \"All agree\"\n",
    "            elif len(unique_answers) == 0:\n",
    "                status = \"No answers\"\n",
    "            else:\n",
    "                status = \"DISAGREE\"\n",
    "            \n",
    "            print(f\"{row['company']}: {status}\")\n",
    "            if status == \"DISAGREE\":\n",
    "                for col in answer_cols:\n",
    "                    provider = col.replace('_answer', '')\n",
    "                    print(f\"  - {provider}: {row.get(col, 'N/A')}\")\n",
    "    else:\n",
    "        print(\"Need at least 2 providers to compare.\")\n",
    "else:\n",
    "    print(\"No comparison data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edge-cases-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 4: Edge Cases (Ambiguous & Unknown Queries)\n",
    "\n",
    "Test cases designed to trigger early exits:\n",
    "- **Ambiguous names**: \"John Smith\" - too many people with this name, should trigger RESPONSE NOT CONFIDENT\n",
    "- **Unknown entities**: \"XYZ Fake Company 12345\" - should trigger ANSWER NOT FOUND\n",
    "- **Common name with specific question**: \"Michael Johnson height\" - many athletes/people named this\n",
    "\n",
    "**Expected behavior**: confidence = 0 for all ambiguous cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edge-case-params",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge case test parameters\n",
    "edge_cases = [\n",
    "    {\"item\": \"John Smith\", \"question\": \"height\", \"format\": \"feet and inches\"},\n",
    "    {\"item\": \"Michael Johnson\", \"question\": \"height\", \"format\": \"feet and inches\"},\n",
    "    {\"item\": \"XYZ Fake Company 12345\", \"question\": \"CEO\", \"format\": \"name\"},\n",
    "    {\"item\": \"David Williams\", \"question\": \"net worth\", \"format\": \"USD amount\"},\n",
    "    {\"item\": \"Apple\", \"question\": \"founder\", \"format\": \"name\"},  # Fruit or company?\n",
    "]\n",
    "\n",
    "print(\"Edge Case Test Items:\")\n",
    "print(\"=\"*60)\n",
    "for i, tc in enumerate(edge_cases, 1):\n",
    "    print(f\"{i}. '{tc['item']}' - {tc['question']} (format: {tc['format']})\")\n",
    "print(\"\\nExpected: Most should return confidence=0 due to ambiguity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-edge-anthropic",
   "metadata": {},
   "outputs": [],
   "source": "if anthropic_api_key:\n    import anthropic\n    \n    print(\"Testing Edge Cases with Anthropic...\\n\")\n    client = anthropic.Anthropic(api_key=anthropic_api_key)\n    user_model = \"claude-sonnet-4-20250514\"\n    \n    edge_results = []\n    \n    for idx, tc in enumerate(tqdm(edge_cases, desc=\"Processing edge cases\")):\n        if idx > 0:\n            time.sleep(time_delay)\n        \n        print(f\"\\n{'#'*80}\")\n        print(f\"# EDGE CASE: {tc['item']} - {tc['question']}\")\n        print(f\"{'#'*80}\")\n        \n        try:\n            result = funnel_of_verification_anthropic(\n                item=tc['item'],\n                search_question=tc['question'],\n                answer_format=tc['format'],\n                additional_instructions=additional_instructions,\n                client=client,\n                user_model=user_model,\n                creativity=creativity,\n                verbose=True  # Enable verbose output\n            )\n            \n            parsed = json.loads(result)\n            edge_results.append({\n                'search_input': tc['item'],\n                'question': tc['question'],\n                'answer': parsed.get('answer', ''),\n                'url': parsed.get('url', ''),\n                'confidence': parsed.get('confidence', ''),\n                'multiple_entities': parsed.get('multiple_entities', '0')\n            })\n            \n            print(f\"\\n>>> FINAL: Answer: {parsed.get('answer')}, Confidence: {parsed.get('confidence')}, Multiple: {parsed.get('multiple_entities')}\")\n            \n        except json.JSONDecodeError as e:\n            print(f\"  JSON Error: {e}\")\n            edge_results.append({\n                'search_input': tc['item'],\n                'question': tc['question'],\n                'answer': f'JSON Error',\n                'url': '',\n                'confidence': '0',\n                'multiple_entities': '0'\n            })\n        except Exception as e:\n            print(f\"  Error: {e}\")\n            edge_results.append({\n                'search_input': tc['item'],\n                'question': tc['question'],\n                'answer': f'Error: {e}',\n                'url': '',\n                'confidence': '0',\n                'multiple_entities': '0'\n            })\n    \n    df_edge_anthropic = pd.DataFrame(edge_results)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"ANTHROPIC EDGE CASE RESULTS\")\n    print(\"=\"*60)\n    display(df_edge_anthropic)\n    \n    # Check confidence distribution\n    print(\"\\nConfidence Distribution:\")\n    print(df_edge_anthropic['confidence'].value_counts())\n    print(\"\\nMultiple Entities Distribution:\")\n    print(df_edge_anthropic['multiple_entities'].value_counts())\nelse:\n    print(\"Skipping: Anthropic API key not found\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-edge-google",
   "metadata": {},
   "outputs": [],
   "source": "if google_api_key:\n    import requests\n    \n    print(\"Testing Edge Cases with Google...\\n\")\n    \n    user_model = \"gemini-2.5-flash\"\n    url = f\"https://generativelanguage.googleapis.com/v1beta/models/{user_model}:generateContent\"\n    headers = {\n        \"x-goog-api-key\": google_api_key,\n        \"Content-Type\": \"application/json\"\n    }\n    \n    def make_google_request(url, headers, payload, max_retries=5):\n        for attempt in range(max_retries):\n            response = requests.post(url, headers=headers, json=payload)\n            if response.status_code == 429:\n                wait_time = 2 ** attempt\n                print(f\"  Rate limited, waiting {wait_time}s...\")\n                time.sleep(wait_time)\n                continue\n            response.raise_for_status()\n            return response.json()\n        raise Exception(\"Max retries exceeded\")\n    \n    edge_results = []\n    \n    for idx, tc in enumerate(tqdm(edge_cases, desc=\"Processing edge cases\")):\n        if idx > 0:\n            time.sleep(time_delay)\n        \n        print(f\"\\n{'#'*80}\")\n        print(f\"# EDGE CASE: {tc['item']} - {tc['question']}\")\n        print(f\"{'#'*80}\")\n        \n        try:\n            result = funnel_of_verification_google(\n                item=tc['item'],\n                search_question=tc['question'],\n                answer_format=tc['format'],\n                additional_instructions=additional_instructions,\n                url=url,\n                headers=headers,\n                creativity=creativity,\n                make_google_request=make_google_request,\n                verbose=True  # Enable verbose output\n            )\n            \n            parsed = json.loads(result)\n            edge_results.append({\n                'search_input': tc['item'],\n                'question': tc['question'],\n                'answer': parsed.get('answer', ''),\n                'url': parsed.get('url', ''),\n                'confidence': parsed.get('confidence', ''),\n                'multiple_entities': parsed.get('multiple_entities', '0')\n            })\n            \n            print(f\"\\n>>> FINAL: Answer: {parsed.get('answer')}, Confidence: {parsed.get('confidence')}, Multiple: {parsed.get('multiple_entities')}\")\n            \n        except json.JSONDecodeError as e:\n            print(f\"  JSON Error: {e}\")\n            edge_results.append({\n                'search_input': tc['item'],\n                'question': tc['question'],\n                'answer': f'JSON Error',\n                'url': '',\n                'confidence': '0',\n                'multiple_entities': '0'\n            })\n        except Exception as e:\n            print(f\"  Error: {e}\")\n            edge_results.append({\n                'search_input': tc['item'],\n                'question': tc['question'],\n                'answer': f'Error: {e}',\n                'url': '',\n                'confidence': '0',\n                'multiple_entities': '0'\n            })\n    \n    df_edge_google = pd.DataFrame(edge_results)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"GOOGLE EDGE CASE RESULTS\")\n    print(\"=\"*60)\n    display(df_edge_google)\n    \n    print(\"\\nConfidence Distribution:\")\n    print(df_edge_google['confidence'].value_counts())\n    print(\"\\nMultiple Entities Distribution:\")\n    print(df_edge_google['multiple_entities'].value_counts())\nelse:\n    print(\"Skipping: Google API key not found\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-edge-perplexity",
   "metadata": {},
   "outputs": [],
   "source": "if perplexity_api_key:\n    from perplexity import Perplexity\n    \n    print(\"Testing Edge Cases with Perplexity...\\n\")\n    client = Perplexity(api_key=perplexity_api_key)\n    user_model = \"sonar\"\n    \n    edge_results = []\n    \n    for idx, tc in enumerate(tqdm(edge_cases, desc=\"Processing edge cases\")):\n        if idx > 0:\n            time.sleep(time_delay)\n        \n        print(f\"\\n{'#'*80}\")\n        print(f\"# EDGE CASE: {tc['item']} - {tc['question']}\")\n        print(f\"{'#'*80}\")\n        \n        try:\n            result = funnel_of_verification_perplexity(\n                item=tc['item'],\n                search_question=tc['question'],\n                answer_format=tc['format'],\n                additional_instructions=additional_instructions,\n                client=client,\n                user_model=user_model,\n                creativity=creativity,\n                verbose=True  # Enable verbose output\n            )\n            \n            parsed = json.loads(result)\n            edge_results.append({\n                'search_input': tc['item'],\n                'question': tc['question'],\n                'answer': parsed.get('answer', ''),\n                'url': parsed.get('url', ''),\n                'confidence': parsed.get('confidence', ''),\n                'multiple_entities': parsed.get('multiple_entities', '0')\n            })\n            \n            print(f\"\\n>>> FINAL: Answer: {parsed.get('answer')}, Confidence: {parsed.get('confidence')}, Multiple: {parsed.get('multiple_entities')}\")\n            \n        except json.JSONDecodeError as e:\n            print(f\"  JSON Error: {e}\")\n            edge_results.append({\n                'search_input': tc['item'],\n                'question': tc['question'],\n                'answer': f'JSON Error',\n                'url': '',\n                'confidence': '0',\n                'multiple_entities': '0'\n            })\n        except Exception as e:\n            print(f\"  Error: {e}\")\n            edge_results.append({\n                'search_input': tc['item'],\n                'question': tc['question'],\n                'answer': f'Error: {e}',\n                'url': '',\n                'confidence': '0',\n                'multiple_entities': '0'\n            })\n    \n    df_edge_perplexity = pd.DataFrame(edge_results)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"PERPLEXITY EDGE CASE RESULTS\")\n    print(\"=\"*60)\n    display(df_edge_perplexity)\n    \n    print(\"\\nConfidence Distribution:\")\n    print(df_edge_perplexity['confidence'].value_counts())\n    print(\"\\nMultiple Entities Distribution:\")\n    print(df_edge_perplexity['multiple_entities'].value_counts())\nelse:\n    print(\"Skipping: Perplexity API key not found\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edge-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare edge case results across providers\n",
    "print(\"=\"*80)\n",
    "print(\"EDGE CASE COMPARISON: Ambiguous Query Handling\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "edge_comparison = []\n",
    "for tc in edge_cases:\n",
    "    row = {'item': tc['item'], 'question': tc['question']}\n",
    "    \n",
    "    if 'df_edge_anthropic' in dir():\n",
    "        match = df_edge_anthropic[df_edge_anthropic['search_input'] == tc['item']]\n",
    "        if not match.empty:\n",
    "            row['anthropic_answer'] = match.iloc[0]['answer'][:30] + '...' if len(str(match.iloc[0]['answer'])) > 30 else match.iloc[0]['answer']\n",
    "            row['anthropic_conf'] = match.iloc[0]['confidence']\n",
    "    \n",
    "    if 'df_edge_google' in dir():\n",
    "        match = df_edge_google[df_edge_google['search_input'] == tc['item']]\n",
    "        if not match.empty:\n",
    "            row['google_answer'] = match.iloc[0]['answer'][:30] + '...' if len(str(match.iloc[0]['answer'])) > 30 else match.iloc[0]['answer']\n",
    "            row['google_conf'] = match.iloc[0]['confidence']\n",
    "    \n",
    "    if 'df_edge_perplexity' in dir():\n",
    "        match = df_edge_perplexity[df_edge_perplexity['search_input'] == tc['item']]\n",
    "        if not match.empty:\n",
    "            row['perplexity_answer'] = match.iloc[0]['answer'][:30] + '...' if len(str(match.iloc[0]['answer'])) > 30 else match.iloc[0]['answer']\n",
    "            row['perplexity_conf'] = match.iloc[0]['confidence']\n",
    "    \n",
    "    edge_comparison.append(row)\n",
    "\n",
    "df_edge_comparison = pd.DataFrame(edge_comparison)\n",
    "display(df_edge_comparison)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"SUMMARY: How well did models handle ambiguity?\")\n",
    "print(\"-\"*40)\n",
    "conf_cols = [c for c in df_edge_comparison.columns if '_conf' in c]\n",
    "for col in conf_cols:\n",
    "    provider = col.replace('_conf', '')\n",
    "    if col in df_edge_comparison.columns:\n",
    "        zeros = (df_edge_comparison[col] == '0').sum()\n",
    "        total = df_edge_comparison[col].notna().sum()\n",
    "        print(f\"{provider}: {zeros}/{total} returned confidence=0 (expected for ambiguous queries)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "**Test 1-3: Standard CEO Queries**\n",
    "- Search question: \"current CEO\"\n",
    "- Answer format: \"name\"\n",
    "- Companies: Microsoft, Apple Inc, Amazon, Google, Meta\n",
    "- Expected: High confidence (1) for all\n",
    "\n",
    "**Test 4: Edge Cases (Ambiguous Queries)**\n",
    "- \"John Smith\" height → Should return confidence=0 (too many people)\n",
    "- \"Michael Johnson\" height → Should return confidence=0 (common name)\n",
    "- \"XYZ Fake Company\" CEO → Should return confidence=0 (doesn't exist)\n",
    "- \"David Williams\" net worth → Should return confidence=0 (too many people)\n",
    "- \"Apple\" founder → May be ambiguous (fruit vs company)\n",
    "\n",
    "**Output Format:**\n",
    "```\n",
    "| search_input | answer         | url                    | confidence |\n",
    "|--------------|----------------|------------------------|------------|\n",
    "| Microsoft    | Satya Nadella  | https://microsoft.com  | 1          |\n",
    "| John Smith   | Information... | (empty)                | 0          |\n",
    "```\n",
    "\n",
    "**Providers Tested:**\n",
    "1. Anthropic - `tool_choice` JSON forcing\n",
    "2. Google - `responseMimeType` JSON forcing  \n",
    "3. Perplexity - `json_schema` JSON forcing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}